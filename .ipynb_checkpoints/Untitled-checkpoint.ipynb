{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "vocabulary length is: 18758\n",
      "The leangth of X_train is 9596\n",
      "The length of x_dev is 1066\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib import learn\n",
    "from data_helpers import load_data_and_labels, clean_str, get_dir\n",
    "import data_helpers\n",
    "\n",
    "from TextCNNClassifier import NN_config, CALC_config, TextCNNClassifier\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "positive_data_file = \"./data/rt-polaritydata/rt-polarity.pos\"\n",
    "negative_data_file = \"./data/rt-polaritydata/rt-polarity.neg\"\n",
    "dev_sample_percentage = 0.1\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(positive_data_file, negative_data_file)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "print('vocabulary length is:',len(vocab_processor.vocabulary_))\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print('The leangth of X_train is {}'.format(len(x_train)))\n",
    "print('The length of x_dev is {}'.format(len(x_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# ----------------  model processing ------------------------------------------\n",
    "#------------------------------------------------------------------------------\n",
    "num_seqs = max_document_length\n",
    "num_classes = 2\n",
    "num_filters = 128\n",
    "filter_steps = [5,6,7]\n",
    "embedding_size = 200\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size    = 128\n",
    "num_epoches   = 20\n",
    "l2_ratio      = 0.0\n",
    "\n",
    "trains = list(zip(x_train, y_train))\n",
    "devs   = list(zip(x_dev,y_dev))\n",
    "\n",
    "config_nn = NN_config(num_seqs \t\t= num_seqs,\n",
    "\t\t\t\t\t  num_classes \t= num_classes,\n",
    "\t\t\t\t\t  num_filters \t= num_filters,\n",
    "\t\t\t\t\t  filter_steps \t= filter_steps,\n",
    "\t\t\t\t\t  embedding_size= embedding_size,\n",
    "\t\t\t\t\t  vocab_size   \t= vocab_size)\n",
    "config_calc = CALC_config(learning_rate = learning_rate,\n",
    "\t\t\t\t\t\t  batch_size \t= batch_size,\n",
    "\t\t\t\t\t\t  num_epoches \t= num_epoches,\n",
    "\t\t\t\t\t\t  l2_ratio \t\t= l2_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class NN_config(object):\n",
    "\tdef __init__(self, vocab_size, num_filters,filter_steps,num_seqs=1000,num_classes=2, embedding_size=200):\n",
    "\t\tself.vocab_size \t= vocab_size\n",
    "\t\tself.num_filters \t= num_filters\n",
    "\t\tself.filter_steps \t= filter_steps\n",
    "\t\tself.num_seqs \t\t= num_seqs\n",
    "\t\tself.num_classes \t= num_classes\n",
    "\t\tself.embedding_size = embedding_size\t\t\n",
    "\t\t\n",
    "class CALC_config(object):\n",
    "\tdef __init__(self, learning_rate=0.0075, batch_size=64, num_epoches=20, l2_ratio=0.0):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.batch_size    = batch_size\n",
    "\t\tself.num_epoches   = num_epoches\n",
    "\t\tself.l2_ratio      = l2_ratio\n",
    "\n",
    "class TextCNNClassifier(object):\n",
    "\t'''\n",
    "\tA class used to define text classifier use convolution network \n",
    "\tthe form of class like keras or scikit-learn\n",
    "\t'''\n",
    "\tdef __init__(self,config_nn, config_calc):\n",
    "\t\t\n",
    "\t\tself.num_seqs = config_nn.num_seqs\n",
    "\t\tself.num_classes = config_nn.num_classes\n",
    "\t\tself.embedding_size = config_nn.embedding_size\n",
    "\t\tself.vocab_size  = config_nn.vocab_size\n",
    "\t\tself.num_filters = config_nn.num_filters\n",
    "\t\tself.filter_steps = config_nn.filter_steps\n",
    "\t\t\n",
    "\t\tself.learning_rate = config_calc.learning_rate\n",
    "\t\tself.batch_size    = config_calc.batch_size\n",
    "\t\tself.num_epoches   = config_calc.num_epoches\n",
    "\t\tself.l2_ratio      = config_calc.l2_ratio\n",
    "\t\t\n",
    "\t\t#tf.reset_default_graph()\n",
    "\t\tself.build_placeholder()\n",
    "\t\tself.build_embedding_layer()\n",
    "\t\tself.build_nn()\n",
    "\t\tself.build_cost()\n",
    "\t\tself.build_optimizer()\n",
    "\t\tself.saver = tf.train.Saver()\n",
    "\t\t\n",
    "\tdef build_placeholder(self):\n",
    "\t\twith tf.name_scope('inputs_to_data'):\n",
    "\t\t\tself.inputs       = tf.placeholder( tf.int32,shape=[None, self.num_seqs],name='inputs')\n",
    "\t\t\tself.targets      = tf.placeholder(tf.float32,shape=[None, self.num_classes], name='targets')\n",
    "\t\t\tself.keep_prob    = tf.placeholder(tf.float32, name='nn_keep_prob')\n",
    "\t\t\tprint('self.inputs.shape:',self.inputs.shape)\n",
    "\t\n",
    "\tdef build_embedding_layer(self):\n",
    "\t\twith tf.device('/cpu:0'),tf.name_scope('embeddings'):\n",
    "\t\t\tembeddings = tf.Variable(tf.truncated_normal(shape=[self.vocab_size,self.embedding_size],stddev=0.1),\\\n",
    "\t\t\t\t\t\tname = 'embeddings')\n",
    "\t\t\tx = tf.nn.embedding_lookup(embeddings, self.inputs)\n",
    "\t\t\tx = tf.expand_dims(x, axis=-1)\n",
    "\t\t\tself.x = tf.cast(x, tf.float32 )\n",
    "\t\t\tprint('x shape is:',self.x.get_shape())\n",
    "\t\t\t\n",
    "\tdef build_nn(self):\n",
    "\t\tconv_out = []\n",
    "\t\tfor i , filter_step in enumerate(self.filter_steps):\n",
    "\t\t\twith tf.name_scope(\"conv-network-%s\"%filter_step):\n",
    "\t\t\t\tfilter_shape = [filter_step,self.embedding_size, 1,self.num_filters]\n",
    "\t\t\t\tfilters = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1), \\\n",
    "\t\t\t\t\t\t  name='filters')\n",
    "\t\t\t\tbias    = tf.Variable(tf.constant(0.0, shape=[self.num_filters]), name='bias')\n",
    "\t\t\t\t# h_conv : shape =batch_szie * (num_seqs-filter_step+1) * 1 * num_filters\n",
    "\t\t\t\th_conv  = tf.nn.conv2d(self.x, \n",
    "\t\t\t\t                    filter=filters,\n",
    "\t\t\t\t\t\t\t\t\tstrides = [1,1,1,1],\n",
    "\t\t\t\t\t\t\t\t\tpadding='VALID',\n",
    "\t\t\t\t\t\t\t\t\tname='hidden_conv')  \n",
    "\t\t\t\th_relu  = tf.nn.relu(tf.nn.bias_add(h_conv,bias),name='relu')\n",
    "\t\t\t\tksize = [1,self.num_seqs-filter_step+1,1,1]\n",
    "\t\t\t\t#h_pooling: shape = batch_size * 1 * 1 * num_filters\n",
    "\t\t\t\th_pooling = tf.nn.max_pool(h_relu, \n",
    "\t\t\t\t\t\t\t\t\t\t   ksize=ksize,\n",
    "\t\t\t\t\t\t\t\t\t\t   strides=[1,1,1,1],\n",
    "\t\t\t\t\t\t\t\t\t\t   padding='VALID',\n",
    "\t\t\t\t\t\t\t\t\t\t   name='pooling')\n",
    "\t\t\t\tconv_out.append(h_pooling)\n",
    "\t\t\t\t\n",
    "\t\tself.tot_filters_units = self.num_filters * len(self.filter_steps)\n",
    "\t\tself.h_pool = tf.concat(conv_out,axis=3)\n",
    "\t\tself.h_pool_flattern =tf.reshape(self.h_pool, shape=[-1, self.tot_filters_units])\n",
    "\t\t\n",
    "\t\twith tf.name_scope('dropout'):\n",
    "\t\t\tself.h_pool_drop = tf.nn.dropout(self.h_pool_flattern, self.keep_prob)\n",
    "\t\t\t\n",
    "\tdef build_cost(self):\n",
    "\t\twith tf.name_scope('cost'):\n",
    "\t\t\tW = tf.get_variable(shape=[self.tot_filters_units, self.num_classes],name='W',\\\n",
    "\t\t\t\t\tinitializer = tf.contrib.layers.xavier_initializer())\n",
    "\t\t\tbias = tf.Variable(tf.constant(0.1, shape=[self.num_classes],name='bias'))\t\t\t\n",
    "\t\t\tself.scores =  tf.nn.xw_plus_b(self.h_pool_drop, W, bias, name='scores')\n",
    "\t\t\tself.predictions = tf.argmax(self.scores,axis=1,name='predictions')\n",
    "\t\t\tl2_loss = tf.constant(0.0,name='l2_loss')\n",
    "\t\t\tl2_loss += tf.nn.l2_loss(W)\n",
    "\t\t\tl2_loss += tf.nn.l2_loss(bias)\t\t\t\n",
    "\t\t\tlosses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.targets)\n",
    "\t\t\tself.loss = tf.reduce_mean(losses) + self.l2_ratio*l2_loss\n",
    "\t\t\t\n",
    "\t\twith tf.name_scope('accuracy'):\n",
    "\t\t\tpred = tf.equal(self.predictions, tf.argmax(self.targets, axis=1))\n",
    "\t\t\tself.accuracy = tf.reduce_mean(tf.cast(pred,tf.float32))\n",
    "\t\t\t\n",
    "\tdef build_optimizer(self):\n",
    "\t\twith tf.name_scope('optimizer'):\n",
    "\t\t\toptimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "\t\t\tgrad_and_vars = optimizer.compute_gradients(self.loss)\n",
    "\t\t\tself.train_op = optimizer.apply_gradients(grad_and_vars)\n",
    "\t\t\t\n",
    "\tdef random_batches(self,data,shuffle=True):\n",
    "\t\tdata = np.array(data)\n",
    "\t\tdata_size = len(data)\n",
    "\t\tnum_batches_per_epoch = int((data_size-1)/self.batch_size) + 1\n",
    "\t\tif shuffle :\n",
    "\t\t\tshuffle_index = np.random.permutation(np.arange(data_size))\n",
    "\t\t\tshuffled_data = data[shuffle_index]\n",
    "\t\telse:\n",
    "\t\t\tshuffled_data = data\n",
    "\t\t#del data\n",
    "\t\tfor epoch in range(self.num_epoches):\n",
    "\t\t\tfor batch_num in range(num_batches_per_epoch):\n",
    "\t\t\t\tstart = batch_num * self.batch_size\n",
    "\t\t\t\tend   = min(start + self.batch_size,data_size)\n",
    "\t\t\t\tyield shuffled_data[start:end]\t\t\n",
    "\t\n",
    "\tdef fit(self,data):\n",
    "\t\t#self.graph = tf.Graph()\n",
    "\t\t#with self.graph.as_default():\n",
    "\t\tself.session = tf.Session()\n",
    "\t\twith self.session as sess:\n",
    "\t\t\t#self.saver = tf.train.Saver(tf.global_variables())\n",
    "\t\t\tsess.run(tf.global_variables_initializer())\n",
    "\t\t\tbatches = self.random_batches(list(data))\n",
    "\t\t\taccuracy_list = []\n",
    "\t\t\tloss_list = []\n",
    "\t\t\t#prediction_list = []\n",
    "\t\t\titerations = 0\n",
    "\t\t\t# model saving\n",
    "\t\t\tsave_path = os.path.abspath(os.path.join(os.path.curdir, 'models'))\t\t\t\t\t\n",
    "\t\t\tif not os.path.exists(save_path):\n",
    "\t\t\t\tos.makedirs(save_path)\n",
    "\t\t\t\n",
    "\t\t\tfor batch in batches:\n",
    "\t\t\t\titerations  += 1\n",
    "\t\t\t\tx_batch, y_batch = zip(*batch)\n",
    "\t\t\t\tx_batch = np.array(x_batch)\n",
    "\t\t\t\ty_batch = np.array(y_batch)\n",
    "\t\t\t\tfeed = { self.inputs: x_batch,\n",
    "\t\t\t\t\t\t self.targets: y_batch,\n",
    "\t\t\t\t\t\t self.keep_prob: 0.5}\n",
    "\t\t\t\tbatch_pred, batch_accuracy, batch_cost, _ = sess.run([self.predictions, self.accuracy,\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.loss, self.train_op], feed_dict=feed)\n",
    "\t\t\t\taccuracy_list.append(batch_accuracy)\n",
    "\t\t\t\tloss_list.append(batch_cost)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif iterations % 10 == 0:\n",
    "\t\t\t\t\tprint('The trainning step is {0}'.format(iterations),\\\n",
    "\t\t\t\t\t\t 'trainning_loss: {:.3f}'.format(loss_list[-1]), \\\n",
    "\t\t\t\t\t\t 'trainning_accuracy: {:.3f}'.format(accuracy_list[-1]))\n",
    "\t\t\t\tif iterations % 100 == 0:\n",
    "\t\t\t\t\tself.saver.save(sess, os.path.join(save_path, 'model'), global_step = iterations)\n",
    "\t\n",
    "\t\t\tself.saver.save(sess, os.path.join(save_path, 'model'), global_step = iterations)\n",
    "\t\n",
    "\tdef load_model(self,start_path=None):\n",
    "\t\tif start_path == None:\n",
    "\t\t\tstart_path = os.path.abspath(os.path.join(os.path.curdir,'models','checkpoint'))\n",
    "\t\t\tprint('default start_path is',start_path)\n",
    "\t\t\t#star = start_path\n",
    "\t\tself.session = tf.Session()\n",
    "\t\tif start_path == None:\n",
    "\t\t\tself.saver.restore(self.session, tf.train.latest_checkpoint(start_path))\n",
    "\t\telse:\n",
    "\t\t\tself.saver.restore(self.session,start_path)\n",
    "\t\tprint('Restored from {} completed'.format(start_path))\n",
    "\t\t\n",
    "\tdef predict_accuracy(self,data,test=True):\n",
    "\t\tsess = self.session\n",
    "\t\titerations = 0\n",
    "\t\taccuracy_list = []\n",
    "\t\tpredictions = []\n",
    "\t\tself.num_epoches = 1\n",
    "\t\tbatches = self.random_batches(data,shuffle=False)\n",
    "\t\tfor batch in batches:\n",
    "\t\t\titerations += 1\n",
    "\t\t\tx_inputs, y_inputs = zip(*batch)\n",
    "\t\t\tx_inputs = np.array(x_inputs)\n",
    "\t\t\ty_inputs = np.array(y_inputs)\n",
    "\t\t\tfeed = {self.inputs: x_inputs,\n",
    "\t\t\t\t\tself.targets: y_inputs,\n",
    "\t\t\t\t\tself.keep_prob: 1.0\t\t\t\n",
    "\t\t\t\t}\n",
    "\t\t\tbatch_pred, batch_accuracy, batch_loss = sess.run([self.predictions,\\\n",
    "\t\t\tself.accuracy, self.loss], feed_dict=feed)\n",
    "\t\t\taccuracy_list.append(batch_accuracy)\n",
    "\t\t\tpredictions.append(batch_pred)\n",
    "\t\t\tprint('The trainning step is {0}'.format(iterations),\\\n",
    "\t\t\t\t 'trainning_accuracy: {:.3f}'.format(accuracy_list[-1]))\t\t\t\n",
    "\t\t\t\n",
    "\t\taccuracy = np.mean(accuracy_list)\n",
    "\t\tpredictions = [list(pred) for pred in predictions]\n",
    "\t\tpredictions = [p for pred in predictions for p in pred]\n",
    "\t\tpredictions = np.array(predictions)\n",
    "\t\tif test :\n",
    "\t\t\treturn predictions, accuracy\n",
    "\t\telse:\n",
    "\t\t\treturn accuracy\n",
    "\t\t\t\n",
    "\tdef predict(self, data):\n",
    "\t\tsess = self.session\n",
    "\t\titerations = 0\n",
    "\t\tpedictions_list = []\n",
    "\t\tself.num_epoches = 1\n",
    "\t\tbatches = self.random_batches(data)\n",
    "\t\tfor batch in batches:\n",
    "\t\t\tx_inputs = batch\n",
    "\t\t\tfeed = {self.inputs : x_inputs,\n",
    "\t\t\t\t\tself.keep_prob:1.0}\n",
    "\t\t\tbatch_pred = sess.run([self.predictions],feed_dict=feed)\n",
    "\t\t\tpredictions_list.append(batch_pred)\n",
    "\t\tpredictions = [list(pred) for pred in predictions]\n",
    "\t\tpredictions = [p for pred in predictions for p in pred]\n",
    "\t\tpredictions = np.array(predictions).reshape(-1,1)\n",
    "\t\treturn predictions\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.inputs.shape: (?, 56)\n",
      "x shape is: (?, 56, 200, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-c85284da37c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_model2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextCNNClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_nn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfig_calc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-ecf3caffe2c0>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config_nn, config_calc)\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_embedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-ecf3caffe2c0>\u001b[0m in \u001b[0;36mbuild_cost\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mbuild_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtot_filters_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m,\u001b[0m                                  \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                         \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bias'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxw_plus_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh_pool_drop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scores'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m           use_resource=use_resource)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    662\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 664\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "text_model = TextCNNClassifier(config_nn,config_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.fit(trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.load_model(\".\\\\models\\\\model-1500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = text_model.predict_accuracy(devs,test=False)\t\n",
    "print('the dev accuracy is :',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
